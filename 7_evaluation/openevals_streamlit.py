import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import numpy as np
import os
from dotenv import load_dotenv
from langchain_anthropic import ChatAnthropic
import openevals
import plotly.express as px
import tempfile
import importlib
import requests
import numpy as np
import boto3
from langchain_aws import BedrockEmbeddings

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="OpenEvals í‰ê°€ ëŒ€ì‹œë³´ë“œ", layout="wide")


# ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ (torch ì—†ì´)
def simple_text_similarity(text1, text2):
    """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (ë‹¨ì–´ ê¸°ë°˜)"""
    # ë‹¨ì–´ ë¶„í•  ë° ì†Œë¬¸ì ë³€í™˜
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())

    # êµì§‘í•©ê³¼ í•©ì§‘í•© ê³„ì‚°
    intersection = words1.intersection(words2)
    union = words1.union(words2)

    # Jaccard ìœ ì‚¬ë„ ê³„ì‚°
    if len(union) == 0:
        return 0.0
    return len(intersection) / len(union)


# í°íŠ¸ ì„¤ì • í•¨ìˆ˜
def setup_korean_font():
    # matplotlib ì°¨íŠ¸ì—ì„œëŠ” í•œê¸€ í°íŠ¸ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©
    try:
        plt.rcParams["font.family"] = "DejaVu Sans"
        plt.rcParams["axes.unicode_minus"] = False
        return True
    except Exception as e:
        print(f"í°íŠ¸ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False


@st.cache_resource
def get_bedrock_embeddings():
    """AWS Bedrock ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ìºì‹œë¨, torch ë¶ˆí•„ìš”)"""
    try:
        return BedrockEmbeddings(
            region_name="us-east-1", model_id="amazon.titan-embed-text-v1"
        )
    except Exception as e:
        st.warning(f"AWS Bedrock ì„ë² ë”© ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None


def calculate_semantic_similarity(text1, text2, embedding_model=None):
    """AWS Bedrock ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚° (torch ë¶ˆí•„ìš”)"""
    try:
        if embedding_model is None:
            embedding_model = get_bedrock_embeddings()

        # AWS ìê²© ì¦ëª…ì´ ì—†ìœ¼ë©´ ë°”ë¡œ fallback
        if embedding_model is None:
            return simple_text_similarity(text1, text2)

        # Bedrock ì„ë² ë”© ìƒì„±
        emb1 = embedding_model.embed_query(text1)
        emb2 = embedding_model.embed_query(text2)

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (numpyë§Œ ì‚¬ìš©)
        emb1 = np.array(emb1)
        emb2 = np.array(emb2)

        dot_product = np.dot(emb1, emb2)
        norm1 = np.linalg.norm(emb1)
        norm2 = np.linalg.norm(emb2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        similarity = dot_product / (norm1 * norm2)
        return max(0.0, min(1.0, similarity))

    except Exception as e:
        # ì²˜ìŒ ì—ëŸ¬ë§Œ í‘œì‹œí•˜ê³  ì´í›„ëŠ” ì¡°ìš©íˆ fallback
        if not hasattr(calculate_semantic_similarity, "_error_shown"):
            st.warning(f"ì„ë² ë”© ê³„ì‚° ì‹¤íŒ¨, ë‹¨ì–´ ê¸°ë°˜ ìœ ì‚¬ë„ë¡œ ëŒ€ì²´: {e}")
            calculate_semantic_similarity._error_shown = True
        return simple_text_similarity(text1, text2)


# OpenEvals í‰ê°€ í•¨ìˆ˜ (ì„ë² ë”© + LLM í•˜ì´ë¸Œë¦¬ë“œ)
def run_openevals_evaluation(eval_data, model_name):
    try:
        # ëª¨ë¸ ì´ˆê¸°í™”
        model = ChatAnthropic(
            model_name=model_name, anthropic_api_key=os.getenv("ANTHROPIC_API_KEY")
        )

        # AWS Bedrock ì„ë² ë”© ë¡œë“œ
        embedding_model = get_bedrock_embeddings()
        if embedding_model is not None:
            st.info(
                "ğŸš€ ê³ í’ˆì§ˆ í•˜ì´ë¸Œë¦¬ë“œ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. (LLM + AWS Bedrock ì„ë² ë”©, torch ë¶ˆí•„ìš”)"
            )
        else:
            st.info("ğŸš€ ê³ í’ˆì§ˆ LLM í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. (torch ë¶ˆí•„ìš”, fallback ì§€ì›)")

        # ë‹¤ì¤‘ ë©”íŠ¸ë¦­ í‰ê°€ í”„ë¡¬í”„íŠ¸
        evaluation_prompts = {
            "accuracy": """ë‹¤ìŒì€ ì§ˆë¬¸ê³¼ ì‘ë‹µ, ê·¸ë¦¬ê³  ì°¸ì¡° ë‹µë³€ì…ë‹ˆë‹¤. ì‘ë‹µì˜ ì •í™•ì„±ì„ 0.0ì—ì„œ 1.0 ì‚¬ì´ì˜ ì ìˆ˜ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}
ì‘ë‹µ: {response}
ì°¸ì¡°: {reference}

ì •í™•ì„± í‰ê°€ ê¸°ì¤€:
- ì‘ë‹µì´ ì°¸ì¡° ë‹µë³€ê³¼ ì–¼ë§ˆë‚˜ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ê°€
- ì‚¬ì‹¤ì  ì˜¤ë¥˜ê°€ ìˆëŠ”ê°€
- í•µì‹¬ ì •ë³´ê°€ ì˜¬ë°”ë¥´ê²Œ ì „ë‹¬ë˜ì—ˆëŠ”ê°€

0.0ì—ì„œ 1.0 ì‚¬ì´ì˜ ìˆ«ìë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”.""",
            "completeness": """ë‹¤ìŒì€ ì§ˆë¬¸ê³¼ ì‘ë‹µ, ê·¸ë¦¬ê³  ì°¸ì¡° ë‹µë³€ì…ë‹ˆë‹¤. ì‘ë‹µì˜ ì™„ì „ì„±ì„ 0.0ì—ì„œ 1.0 ì‚¬ì´ì˜ ì ìˆ˜ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}
ì‘ë‹µ: {response}
ì°¸ì¡°: {reference}

ì™„ì „ì„± í‰ê°€ ê¸°ì¤€:
- ì§ˆë¬¸ì— ëŒ€í•´ ì¶©ë¶„íˆ ë‹µë³€í•˜ê³  ìˆëŠ”ê°€
- ì¤‘ìš”í•œ ì •ë³´ê°€ ëˆ„ë½ë˜ì§€ ì•Šì•˜ëŠ”ê°€
- ì°¸ì¡° ë‹µë³€ê³¼ ë¹„êµí–ˆì„ ë•Œ í•„ìš”í•œ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€

0.0ì—ì„œ 1.0 ì‚¬ì´ì˜ ìˆ«ìë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”.""",
            "relevance": """ë‹¤ìŒì€ ì§ˆë¬¸ê³¼ ì‘ë‹µ, ê·¸ë¦¬ê³  ì°¸ì¡° ë‹µë³€ì…ë‹ˆë‹¤. ì‘ë‹µì˜ ê´€ë ¨ì„±ì„ 0.0ì—ì„œ 1.0 ì‚¬ì´ì˜ ì ìˆ˜ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}
ì‘ë‹µ: {response}
ì°¸ì¡°: {reference}

ê´€ë ¨ì„± í‰ê°€ ê¸°ì¤€:
- ì‘ë‹µì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ê°€
- ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì€ê°€
- ì§ˆë¬¸ì˜ ì˜ë„ì— ë§ê²Œ ë‹µë³€í•˜ê³  ìˆëŠ”ê°€

0.0ì—ì„œ 1.0 ì‚¬ì´ì˜ ìˆ«ìë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”.""",
            "clarity": """ë‹¤ìŒì€ ì§ˆë¬¸ê³¼ ì‘ë‹µ, ê·¸ë¦¬ê³  ì°¸ì¡° ë‹µë³€ì…ë‹ˆë‹¤. ì‘ë‹µì˜ ëª…í™•ì„±ì„ 0.0ì—ì„œ 1.0 ì‚¬ì´ì˜ ì ìˆ˜ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}
ì‘ë‹µ: {response}
ì°¸ì¡°: {reference}

ëª…í™•ì„± í‰ê°€ ê¸°ì¤€:
- ì‘ë‹µì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±ë˜ì—ˆëŠ”ê°€
- ë¬¸ì¥ êµ¬ì¡°ê°€ ëª…í™•í•œê°€
- ëª¨í˜¸í•˜ê±°ë‚˜ í˜¼ë€ìŠ¤ëŸ¬ìš´ í‘œí˜„ì´ ì—†ëŠ”ê°€

0.0ì—ì„œ 1.0 ì‚¬ì´ì˜ ìˆ«ìë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”.""",
        }

        # ë‹¤ì¤‘ ë©”íŠ¸ë¦­ í‰ê°€ ì‹¤í–‰ (LLM + ì„ë² ë”© í•˜ì´ë¸Œë¦¬ë“œ)
        metrics = [
            "accuracy",
            "completeness",
            "relevance",
            "clarity",
            "semantic_similarity",
        ]
        all_results = []

        for data in eval_data:
            result_dict = {
                "question": data["question"],
                "response": data["response"],
                "reference": data["reference"],
            }

            # ê° ë©”íŠ¸ë¦­ë³„ë¡œ í‰ê°€ ì‹¤í–‰
            for metric in metrics:
                if metric == "semantic_similarity":
                    # AWS Bedrock ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
                    similarity_score = calculate_semantic_similarity(
                        data["response"], data["reference"], embedding_model
                    )
                    result_dict[metric] = similarity_score
                else:
                    # LLM ê¸°ë°˜ í‰ê°€ (ê³ í’ˆì§ˆ, API í‚¤ í•„ìš”)
                    try:
                        judge = openevals.create_llm_as_judge(
                            prompt=evaluation_prompts[metric], judge=model
                        )

                        result = judge(
                            question=data["question"],
                            response=data["response"],
                            reference=data["reference"],
                        )

                        # ì ìˆ˜ ì¶”ì¶œ (ê°•í™”ëœ ì•ˆì „í•œ ë°©ì‹)
                        llm_score = 0.5  # ê¸°ë³¸ê°’

                        import re

                        # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì ìˆ˜ ì°¾ê¸°
                        if isinstance(result, dict):
                            # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° score í‚¤ ì°¾ê¸°
                            if "score" in result:
                                score_text = str(result["score"])
                            else:
                                # ì „ì²´ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                                score_text = str(result)
                        else:
                            # ê²°ê³¼ ì „ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                            score_text = str(result)

                        # ìˆ«ì íŒ¨í„´ ì°¾ê¸° (0.0 ~ 1.0 ë²”ìœ„)
                        numbers = re.findall(r"0?\.\d+|1\.0|1|0", score_text)
                        if numbers:
                            try:
                                extracted_score = float(numbers[0])
                                llm_score = max(0.0, min(1.0, extracted_score))
                            except ValueError:
                                llm_score = 0.5

                    except Exception as eval_error:
                        # LLM í‰ê°€ ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ fallback
                        if not hasattr(run_openevals_evaluation, "_llm_fallback_shown"):
                            st.warning(
                                f"LLM í‰ê°€ ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´: {eval_error}"
                            )
                            run_openevals_evaluation._llm_fallback_shown = True

                        # Fallback to text-based evaluation
                        if metric == "accuracy":
                            llm_score = simple_text_similarity(
                                data["response"], data["reference"]
                            )
                        elif metric == "relevance":
                            llm_score = simple_text_similarity(
                                data["question"], data["response"]
                            )
                        elif metric == "completeness":
                            response_len = len(data["response"].split())
                            reference_len = len(data["reference"].split())
                            length_ratio = min(
                                response_len / max(reference_len, 1), 1.0
                            )
                            content_sim = simple_text_similarity(
                                data["response"], data["reference"]
                            )
                            llm_score = (length_ratio + content_sim) / 2
                        elif metric == "clarity":
                            sentences = (
                                data["response"].count(".")
                                + data["response"].count("!")
                                + data["response"].count("?")
                            )
                            words = len(data["response"].split())
                            if sentences == 0:
                                sentences = 1
                            avg_words_per_sentence = words / sentences
                            clarity_score = 1.0 - abs(avg_words_per_sentence - 15) / 30
                            llm_score = max(0.1, min(1.0, clarity_score))
                        else:
                            llm_score = 0.6

                    # íŠ¹ì • ë©”íŠ¸ë¦­ì— ëŒ€í•´ Bedrock ì„ë² ë”© ìœ ì‚¬ë„ì™€ ê²°í•©
                    if metric in ["accuracy", "relevance"]:
                        # AWS Bedrock ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚°
                        if metric == "accuracy":
                            embedding_score = calculate_semantic_similarity(
                                data["response"], data["reference"], embedding_model
                            )
                        else:  # relevance
                            embedding_score = calculate_semantic_similarity(
                                data["question"], data["response"], embedding_model
                            )

                        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: LLM 70% + Bedrock ì„ë² ë”© 30%
                        hybrid_score = 0.7 * llm_score + 0.3 * embedding_score
                        result_dict[metric] = hybrid_score
                    else:
                        result_dict[metric] = llm_score

            all_results.append(result_dict)

        # ê²°ê³¼ ì²˜ë¦¬ (ì´ë¯¸ all_resultsì— ëª¨ë“  ë°ì´í„°ê°€ í¬í•¨ë¨)
        return all_results
    except Exception as e:
        st.error(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        raise e


# ë©”ì¸ ì•±
def main():
    st.title("OpenEvals í‰ê°€ ëŒ€ì‹œë³´ë“œ")

    tab1, tab2, tab3 = st.tabs(["í‰ê°€ ê²°ê³¼", "í‰ê°€ ì‹¤í–‰", "ì„¤ì •"])

    # íƒ­ 1: í‰ê°€ ê²°ê³¼ í‘œì‹œ
    with tab1:
        st.header("í‰ê°€ ê²°ê³¼")

        # í‰ê°€ ê²°ê³¼ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        result_files = [
            f
            for f in os.listdir(".")
            if f.startswith("evaluation_results_openevals_") and f.endswith(".csv")
        ]

        if result_files:
            # íŒŒì¼ëª…ì—ì„œ ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ
            model_names = [
                f.replace("evaluation_results_openevals_", "").replace(".csv", "")
                for f in result_files
            ]

            # ê²°ê³¼ íŒŒì¼ ì„ íƒ
            if "current_model" in st.session_state:
                default_index = (
                    model_names.index(st.session_state["current_model"])
                    if st.session_state["current_model"] in model_names
                    else 0
                )
            else:
                default_index = 0

            selected_model = st.selectbox("ëª¨ë¸ ì„ íƒ", model_names, index=default_index)

            selected_file = f"evaluation_results_openevals_{selected_model}.csv"

            if os.path.exists(selected_file):
                df = pd.read_csv(selected_file)
                # NaN ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´
                df = df.fillna("")
                st.success(f"'{selected_model}' ëª¨ë¸ì˜ í‰ê°€ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")
            else:
                st.warning(
                    "í‰ê°€ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. 'í‰ê°€ ì‹¤í–‰' íƒ­ì—ì„œ í‰ê°€ë¥¼ ì‹¤í–‰í•´ ì£¼ì„¸ìš”."
                )
                df = None
        else:
            st.warning(
                "í‰ê°€ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. 'í‰ê°€ ì‹¤í–‰' íƒ­ì—ì„œ í‰ê°€ë¥¼ ì‹¤í–‰í•´ ì£¼ì„¸ìš”."
            )
            df = None

        results_available = df is not None

        if results_available:
            # ê²°ê³¼ ê°œìš”
            col1, col2 = st.columns(2)

            with col1:
                st.subheader("í‰ê·  ë©”íŠ¸ë¦­ ì ìˆ˜")

                # ë©”íŠ¸ë¦­ ì»¬ëŸ¼ í™•ì¸
                metric_cols = [
                    "accuracy",
                    "completeness",
                    "relevance",
                    "clarity",
                    "semantic_similarity",
                ]
                available_metrics = [col for col in metric_cols if col in df.columns]

                if available_metrics:
                    # í‰ê·  ì ìˆ˜ ê³„ì‚°
                    avg_scores = df[available_metrics].mean()

                    # ë©”íŠ¸ë¦­ ì´ë¦„ ë§¤í•‘
                    metric_names = {
                        "accuracy": "ì •í™•ì„±",
                        "completeness": "ì™„ì „ì„±",
                        "relevance": "ê´€ë ¨ì„±",
                        "clarity": "ëª…í™•ì„±",
                        "semantic_similarity": "ì˜ë¯¸ì  ìœ ì‚¬ë„",
                    }

                    # ë°” ì°¨íŠ¸
                    fig = px.bar(
                        x=[metric_names.get(m, m) for m in available_metrics],
                        y=avg_scores.values,
                        title=f"Average Metric Scores - {selected_model}",
                        labels={"x": "Metric", "y": "Score"},
                        range_y=[0, 1],
                    )
                    fig.update_layout(showlegend=False, height=400)
                    st.plotly_chart(fig, use_container_width=True)

                    # í‰ê·  ì ìˆ˜ í…Œì´ë¸”
                    score_df = pd.DataFrame(
                        {
                            "Metric": [
                                metric_names.get(m, m) for m in available_metrics
                            ],
                            "Average Score": avg_scores.values,
                        }
                    )
                    st.table(score_df.set_index("Metric").T)
                else:
                    st.warning("ë©”íŠ¸ë¦­ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            with col2:
                st.subheader("ì „ì²´ ê²°ê³¼")

                if available_metrics:
                    # íˆíŠ¸ë§µ ìƒì„±
                    fig, ax = plt.subplots(figsize=(10, 6))

                    # íˆíŠ¸ë§µ ë°ì´í„° ì¤€ë¹„
                    heatmap_data = df[available_metrics].values

                    # ì§ˆë¬¸ ë¼ë²¨ ì¤€ë¹„ (ì˜ì–´ë¡œ í‘œì‹œ)
                    questions = df["question"].tolist()
                    short_questions = [f"Q{i+1}" for i in range(len(questions))]

                    im = ax.imshow(heatmap_data, cmap="Blues", vmin=0, vmax=1)

                    # y ì¶•ì— ì§ˆë¬¸ í‘œì‹œ
                    ax.set_yticks(np.arange(len(questions)))
                    ax.set_yticklabels(short_questions)

                    # x ì¶•ì— ë©”íŠ¸ë¦­ í‘œì‹œ (ì˜ì–´ë¡œ)
                    metric_names_en = {
                        "accuracy": "Accuracy",
                        "completeness": "Completeness",
                        "relevance": "Relevance",
                        "clarity": "Clarity",
                        "semantic_similarity": "Semantic Similarity",
                    }
                    ax.set_xticks(np.arange(len(available_metrics)))
                    ax.set_xticklabels(
                        [metric_names_en.get(m, m) for m in available_metrics],
                        rotation=45,
                    )

                    # ìƒ‰ìƒ ë°”
                    cbar = ax.figure.colorbar(im, ax=ax)
                    cbar.ax.set_ylabel("Score", rotation=-90, va="bottom")

                    # ê° ì…€ì— ê°’ í‘œì‹œ
                    for i in range(len(questions)):
                        for j in range(len(available_metrics)):
                            text = ax.text(
                                j,
                                i,
                                f"{heatmap_data[i, j]:.2f}",
                                ha="center",
                                va="center",
                                color="white" if heatmap_data[i, j] > 0.5 else "black",
                            )

                    plt.title("Metric Scores by Question")
                    plt.tight_layout()

                    st.pyplot(fig)
                else:
                    st.warning("íˆíŠ¸ë§µì„ ìƒì„±í•  ë©”íŠ¸ë¦­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # ìƒì„¸ ê²°ê³¼
            st.subheader("ìƒì„¸ í‰ê°€ ê²°ê³¼")

            # í•„ìš”í•œ ì—´ì´ ìˆëŠ”ì§€ í™•ì¸
            display_cols = ["question", "response", "reference"] + available_metrics

            # ì¡´ì¬í•˜ëŠ” ì—´ë§Œ ì„ íƒ
            available_cols = [col for col in display_cols if col in df.columns]

            if available_cols:
                display_df = df[available_cols]

                # ì—´ ì´ë¦„ ë§¤í•‘
                col_mapping = {
                    "question": "ì§ˆë¬¸",
                    "response": "AI ì‘ë‹µ",
                    "reference": "ì°¸ì¡° ì •ë‹µ",
                    "accuracy": "ì •í™•ì„±",
                    "completeness": "ì™„ì „ì„±",
                    "relevance": "ê´€ë ¨ì„±",
                    "clarity": "ëª…í™•ì„±",
                    "semantic_similarity": "ì˜ë¯¸ì  ìœ ì‚¬ë„",
                }

                # ì¡´ì¬í•˜ëŠ” ì—´ì— ëŒ€í•´ì„œë§Œ ì´ë¦„ ë³€ê²½
                rename_mapping = {
                    col: col_mapping.get(col, col) for col in available_cols
                }
                display_df = display_df.rename(columns=rename_mapping)

                # ë°ì´í„°í”„ë ˆì„ í‘œì‹œ
                st.dataframe(display_df, use_container_width=True)

                # ì§ˆë¬¸ë³„ ìƒì„¸ ê²°ê³¼ í‘œì‹œ
                st.subheader("ì§ˆë¬¸ë³„ ìƒì„¸ ê²°ê³¼")
                for i in range(len(display_df)):
                    question_text = str(display_df.iloc[i]["ì§ˆë¬¸"])
                    with st.expander(
                        f"ì§ˆë¬¸ {i+1}: {question_text[:50]}...",
                        expanded=False,
                    ):
                        col1, col2 = st.columns(2)

                        with col1:
                            st.markdown("**ì§ˆë¬¸:**")
                            st.text(display_df.iloc[i]["ì§ˆë¬¸"])

                            st.markdown("**AI ì‘ë‹µ:**")
                            st.text(display_df.iloc[i]["AI ì‘ë‹µ"])

                        with col2:
                            st.markdown("**ì°¸ì¡° ì •ë‹µ:**")
                            st.text(display_df.iloc[i]["ì°¸ì¡° ì •ë‹µ"])

                            # ë©”íŠ¸ë¦­ ì ìˆ˜ë“¤ í‘œì‹œ
                            st.markdown("**í‰ê°€ ì ìˆ˜:**")

                            # ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ í‘œì‹œ
                            metric_korean_names = {
                                "ì •í™•ì„±": "accuracy",
                                "ì™„ì „ì„±": "completeness",
                                "ê´€ë ¨ì„±": "relevance",
                                "ëª…í™•ì„±": "clarity",
                                "ì˜ë¯¸ì  ìœ ì‚¬ë„": "semantic_similarity",
                            }

                            available_metric_names = [
                                name
                                for name in metric_korean_names.keys()
                                if name in display_df.columns
                            ]

                            if available_metric_names:
                                # 2ê°œì”© ë¬¶ì–´ì„œ í‘œì‹œ
                                metric_rows = [
                                    available_metric_names[i : i + 2]
                                    for i in range(0, len(available_metric_names), 2)
                                ]

                                for metric_row in metric_rows:
                                    metric_cols = st.columns(len(metric_row))
                                    for j, metric_name in enumerate(metric_row):
                                        value = float(display_df.iloc[i][metric_name])

                                        # ìƒ‰ìƒ ì„¤ì •
                                        if value >= 0.8:
                                            delta_color = "normal"  # ì´ˆë¡ìƒ‰ (ìš°ìˆ˜)
                                        elif value < 0.6:
                                            delta_color = "inverse"  # ë¹¨ê°„ìƒ‰ (ë¯¸í¡)
                                        else:
                                            delta_color = "off"  # ì£¼í™©ìƒ‰ (ì–‘í˜¸)

                                        metric_cols[j].metric(
                                            label=metric_name,
                                            value=f"{value:.2f}",
                                            delta=f"ë²”ìœ„: 0-1",
                                            delta_color=delta_color,
                                        )

                                # í‰ê·  ì ìˆ˜ ê³„ì‚° ë° í‘œì‹œ
                                avg_score = sum(
                                    [
                                        float(display_df.iloc[i][name])
                                        for name in available_metric_names
                                    ]
                                ) / len(available_metric_names)
                                st.markdown("---")

                                # í‰ê·  ì ìˆ˜ ë“±ê¸‰
                                if avg_score >= 0.8:
                                    grade = "ìš°ìˆ˜"
                                    grade_color = "normal"
                                elif avg_score >= 0.6:
                                    grade = "ì–‘í˜¸"
                                    grade_color = "off"
                                elif avg_score >= 0.4:
                                    grade = "ë³´í†µ"
                                    grade_color = "off"
                                elif avg_score >= 0.2:
                                    grade = "ë¯¸í¡"
                                    grade_color = "inverse"
                                else:
                                    grade = "ë¶€ì ì ˆ"
                                    grade_color = "inverse"

                                st.metric(
                                    label="ì¢…í•© ì ìˆ˜",
                                    value=f"{avg_score:.2f}",
                                    delta=f"{grade} ({len(available_metric_names)}ê°œ ë©”íŠ¸ë¦­ í‰ê· )",
                                    delta_color=grade_color,
                                )
                            else:
                                st.warning("í‰ê°€ ì ìˆ˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # íƒ­ 2: í‰ê°€ ì‹¤í–‰
    with tab2:
        st.header("OpenEvals í‰ê°€ ì‹¤í–‰")

        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
        AVAILABLE_MODELS = {
            "Claude 3 Haiku": "claude-3-haiku-20240307",
            "Claude 3 Sonnet": "claude-3-sonnet-20240229",
            "Claude 3 Opus": "claude-3-opus-20240229",
        }

        # ëª¨ë¸ ì„ íƒ
        selected_model = st.selectbox(
            "í‰ê°€ì— ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”", list(AVAILABLE_MODELS.keys())
        )

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì…ë ¥
        with st.expander("í…ŒìŠ¤íŠ¸ ë°ì´í„°", expanded=not results_available):
            # ê¸°ë³¸ ìƒ˜í”Œ ë°ì´í„°
            default_data = {
                "question": [
                    "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?",
                    "ì¸ê³µì§€ëŠ¥ì˜ ì¢…ë¥˜ëŠ” ë¬´ì—‡ì´ ìˆë‚˜ìš”?",
                    "ìµœê·¼ 5ë…„ê°„ í•œêµ­ ê²½ì œ ì„±ì¥ë¥ ì€ ì–´ë–»ê²Œ ë³€í–ˆë‚˜ìš”?",
                    "ì˜í™” 'ê¸°ìƒì¶©'ì˜ ê°ë…ì€ ëˆ„êµ¬ì¸ê°€ìš”?",
                    "2023ë…„ ë¯¸êµ­ ì£¼ì‹ ì‹œì¥ì€ ì–´ë• ë‚˜ìš”?",
                ],
                "response": [
                    "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.",
                    "ì¸ê³µì§€ëŠ¥ì€ í¬ê²Œ ê°•ì¸ê³µì§€ëŠ¥ê³¼ ì•½ì¸ê³µì§€ëŠ¥ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í˜„ì¬ ê°œë°œëœ AIëŠ” ëŒ€ë¶€ë¶„ ì•½ì¸ê³µì§€ëŠ¥ì— ì†í•©ë‹ˆë‹¤.",
                    "í•œêµ­ì˜ ê²½ì œ ì„±ì¥ë¥ ì€ 2019ë…„ 2.0%, 2020ë…„ -1.0%, 2021ë…„ 4.0%, 2022ë…„ 2.5%ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. 2023ë…„ì€ ì•„ì§ ì§‘ê³„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                    "ì˜í™” 'ê¸°ìƒì¶©'ì€ ë´‰ì¤€í˜¸ ê°ë…ì´ ì—°ì¶œí–ˆìŠµë‹ˆë‹¤.",
                    "2023ë…„ ë¯¸êµ­ ì£¼ì‹ ì‹œì¥ì€ ìƒìŠ¹ì„¸ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê¸ˆë¦¬ ì¸ìƒ ë“±ì˜ ì˜í–¥ìœ¼ë¡œ ë³€ë™ì„±ì´ í° í•œ í•´ì˜€ìŠµë‹ˆë‹¤.",
                ],
                "reference": [
                    "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸íŠ¹ë³„ì‹œì´ë‹¤.",
                    "ì¸ê³µì§€ëŠ¥ì€ ëª©í‘œë‚˜ ëŠ¥ë ¥ì— ë”°ë¼ ê°•ì¸ê³µì§€ëŠ¥ê³¼ ì•½ì¸ê³µì§€ëŠ¥ìœ¼ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤.",
                    "í•œêµ­ì€í–‰ ê²½ì œí†µê³„ì‹œìŠ¤í…œì— ë”°ë¥´ë©´, í•œêµ­ì˜ ê²½ì œ ì„±ì¥ë¥ ì€ 2019ë…„ 2.0%, 2020ë…„ -1.0%, 2021ë…„ 4.0%, 2022ë…„ 2.5%ë¥¼ ê¸°ë¡í–ˆë‹¤.",
                    "ì˜í™” 'ê¸°ìƒì¶©'ì€ ë´‰ì¤€í˜¸ ê°ë…ì´ ê°ë³¸ê³¼ ì—°ì¶œì„ ë§¡ì•˜ë‹¤.",
                    "2023ë…„ ë¯¸êµ­ ì£¼ì‹ ì‹œì¥ì€ S&P 500 ê¸°ì¤€ìœ¼ë¡œ 20% ì´ìƒ ìƒìŠ¹í–ˆë‹¤.",
                ],
            }

            st.subheader("í…ŒìŠ¤íŠ¸ ë°ì´í„° ì…ë ¥")
            st.info(
                "ê¸°ë³¸ê°’ì´ ì œê³µëœ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )

            # ë°ì´í„° ì´ˆê¸°í™” ë° ë¶ˆëŸ¬ì˜¤ê¸° ë²„íŠ¼
            button_container = st.container()
            with button_container:
                col1, col2 = st.columns([1, 1], gap="small")
                with col1:
                    if st.button(
                        "ì „ì²´ ë°ì´í„° ì§€ìš°ê¸°", type="secondary", use_container_width=True
                    ):
                        # í˜„ì¬ í•­ëª© ìˆ˜ ì €ì¥
                        current_n_items = st.session_state.get("n_items", 5)
                        # session_state ì™„ì „ ì´ˆê¸°í™”
                        st.session_state.clear()
                        # í•„ìš”í•œ ìƒíƒœë§Œ ë‹¤ì‹œ ì„¤ì •
                        st.session_state["n_items"] = current_n_items
                        st.session_state["clear_data"] = True
                        # ëª¨ë“  ì…ë ¥ í•„ë“œë¥¼ ë¹ˆ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
                        for i in range(10):  # ìµœëŒ€ ê°€ëŠ¥í•œ í•­ëª© ìˆ˜
                            st.session_state[f"q_{i}"] = ""
                            st.session_state[f"r_{i}"] = ""
                            st.session_state[f"ref_{i}"] = ""
                        st.rerun()
                with col2:
                    if st.button(
                        "ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°",
                        type="secondary",
                        use_container_width=True,
                    ):
                        # í˜„ì¬ í•­ëª© ìˆ˜ ì €ì¥
                        current_n_items = st.session_state.get("n_items", 5)
                        # session_state ì™„ì „ ì´ˆê¸°í™”
                        st.session_state.clear()
                        # í•„ìš”í•œ ìƒíƒœë§Œ ë‹¤ì‹œ ì„¤ì •
                        st.session_state["n_items"] = current_n_items
                        st.session_state["clear_data"] = False
                        st.rerun()

            # ì—¬ëŸ¬ í•­ëª© ì…ë ¥ì„ ìœ„í•œ ì»¨í…Œì´ë„ˆ
            if "n_items" not in st.session_state:
                st.session_state["n_items"] = 5

            n_items = st.number_input(
                "í…ŒìŠ¤íŠ¸ í•­ëª© ìˆ˜",
                min_value=1,
                max_value=10,
                value=st.session_state["n_items"],
            )
            if n_items != st.session_state["n_items"]:
                st.session_state["n_items"] = n_items
                st.rerun()

            test_data = {"question": [], "response": [], "reference": []}

            for i in range(n_items):
                st.markdown(f"### í…ŒìŠ¤íŠ¸ í•­ëª© {i+1}")
                col1, col2 = st.columns(2)

                # ê° í•„ë“œì˜ í‚¤ ìƒì„±
                q_key = f"q_{i}"
                r_key = f"r_{i}"
                ref_key = f"ref_{i}"

                if "clear_data" in st.session_state and st.session_state["clear_data"]:
                    # ì „ì²´ ë°ì´í„° ì§€ìš°ê¸°ê°€ í™œì„±í™”ëœ ê²½ìš°
                    st.session_state[q_key] = ""
                    st.session_state[r_key] = ""
                    st.session_state[ref_key] = ""
                    default_q = ""
                    default_r = ""
                    default_ref = ""
                else:
                    # ê¸°ë³¸ ë°ì´í„° ì‚¬ìš©
                    default_q = (
                        default_data["question"][i]
                        if i < len(default_data["question"])
                        else ""
                    )
                    default_r = (
                        default_data["response"][i]
                        if i < len(default_data["response"])
                        else ""
                    )
                    default_ref = (
                        default_data["reference"][i]
                        if i < len(default_data["reference"])
                        else ""
                    )

                with col1:
                    q = st.text_area(f"ì§ˆë¬¸ {i+1}", value=default_q, key=q_key)
                    r = st.text_area(f"ì‘ë‹µ {i+1}", value=default_r, key=r_key)

                with col2:
                    ref = st.text_area(
                        f"ì°¸ì¡° ì •ë‹µ {i+1}", value=default_ref, key=ref_key
                    )

                if q and r and ref:
                    test_data["question"].append(q)
                    test_data["response"].append(r)
                    test_data["reference"].append(ref)

            if len(test_data["question"]) > 0:
                st.success(f"{len(test_data['question'])}ê°œ í•­ëª©ì´ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                st.warning("ìœ íš¨í•œ í…ŒìŠ¤íŠ¸ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  í•„ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

        # í‰ê°€ ì‹¤í–‰ ë²„íŠ¼
        if st.button("í‰ê°€ ì‹¤í–‰", type="primary"):
            with st.spinner("í‰ê°€ë¥¼ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    # í‰ê°€ ë°ì´í„° ì¤€ë¹„
                    eval_data = []
                    for q, r, ref in zip(
                        test_data["question"],
                        test_data["response"],
                        test_data["reference"],
                    ):
                        eval_data.append(
                            {"question": q, "response": r, "reference": ref}
                        )

                    # í‰ê°€ ì‹¤í–‰
                    results = run_openevals_evaluation(
                        eval_data=eval_data, model_name=AVAILABLE_MODELS[selected_model]
                    )

                    if results:
                        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                        df = pd.DataFrame(results)

                        # ëª¨ë¸ ì •ë³´ ì¶”ê°€
                        df["model"] = selected_model

                        # ê²°ê³¼ íŒŒì¼ëª… ìƒì„±
                        result_file = f"evaluation_results_openevals_{selected_model.lower().replace(' ', '_')}.csv"

                        # ê²°ê³¼ ì €ì¥
                        df.to_csv(result_file, index=False)

                        # ëª¨ë¸ ì •ë³´ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                        st.session_state["current_model"] = selected_model
                        st.session_state["current_results_file"] = result_file

                        st.success(
                            f"í‰ê°€ ì™„ë£Œ! '{selected_model}' ëª¨ë¸ í‰ê°€ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. 'í‰ê°€ ê²°ê³¼' íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”."
                        )
                        st.rerun()
                    else:
                        st.error("í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.error(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

    # íƒ­ 3: ì„¤ì •
    with tab3:
        st.header("ì„¤ì •")

        # í‰ê°€ ê¸°ì¤€ ì„¤ëª…
        with st.expander("OpenEvals í‰ê°€ ê¸°ì¤€ ì„¤ëª…", expanded=True):
            st.markdown(
                """
            ### OpenEvals í•˜ì´ë¸Œë¦¬ë“œ ë‹¤ì¤‘ ë©”íŠ¸ë¦­ í‰ê°€ ì„¤ëª…
            
            * **5ê°€ì§€ í‰ê°€ ë©”íŠ¸ë¦­**: ëª¨ë¸ì˜ ì‘ë‹µì„ ë‹¤ìŒ 5ê°œ ê¸°ì¤€ìœ¼ë¡œ ê°ê° 0.0-1.0 ì ìˆ˜ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
              - **ì •í™•ì„±(Accuracy)**: ì‘ë‹µì´ ì°¸ì¡° ë‹µë³€ê³¼ ì–¼ë§ˆë‚˜ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ê°€ (LLM + ì„ë² ë”© í•˜ì´ë¸Œë¦¬ë“œ)
              - **ì™„ì „ì„±(Completeness)**: ì‘ë‹µì´ ì§ˆë¬¸ì— ëŒ€í•´ ì¶©ë¶„íˆ ë‹µë³€í•˜ê³  ìˆëŠ”ê°€ (LLM ê¸°ë°˜)
              - **ê´€ë ¨ì„±(Relevance)**: ì‘ë‹µì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ê°€ (LLM + ì„ë² ë”© í•˜ì´ë¸Œë¦¬ë“œ)
              - **ëª…í™•ì„±(Clarity)**: ì‘ë‹µì´ ì´í•´í•˜ê¸° ì‰½ê³  ëª…í™•í•˜ê²Œ ì‘ì„±ë˜ì—ˆëŠ”ê°€ (LLM ê¸°ë°˜)
              - **ì˜ë¯¸ì  ìœ ì‚¬ë„(Semantic Similarity)**: ì‘ë‹µê³¼ ì°¸ì¡° ë‹µë³€ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ (ì„ë² ë”© ê¸°ë°˜)
            
            * **í•˜ì´ë¸Œë¦¬ë“œ í‰ê°€ ë°©ì‹**:
              - **LLM í‰ê°€**: Claude ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ë¶„ì„í•˜ì—¬ ì ìˆ˜ ì‚°ì¶œ
              - **ì„ë² ë”© í‰ê°€**: SentenceTransformerë¡œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
              - **í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜**: ì •í™•ì„±/ê´€ë ¨ì„±ì€ LLM 70% + ì„ë² ë”© 30% ê°€ì¤‘í‰ê· 
            
            * **ì ìˆ˜ í•´ì„**:
              - **0.8-1.0**: ìš°ìˆ˜í•œ ë‹µë³€ (ëŒ€ë¶€ë¶„ ì •í™•í•˜ê³  ì™„ì „í•¨)
              - **0.6-0.7**: ì–‘í˜¸í•œ ë‹µë³€ (ë¶€ë¶„ì ìœ¼ë¡œ ì •í™•í•˜ê±°ë‚˜ ë¶ˆì™„ì „í•¨)
              - **0.4-0.5**: ë³´í†µ ë‹µë³€ (ì¼ë¶€ ì •í™•í•˜ì§€ë§Œ ìƒë‹¹í•œ ì˜¤ë¥˜ë‚˜ ëˆ„ë½)
              - **0.2-0.3**: ë¯¸í¡í•œ ë‹µë³€ (ëŒ€ë¶€ë¶„ ë¶€ì •í™•í•˜ê±°ë‚˜ ê´€ë ¨ì„± ë‚®ìŒ)
              - **0.0-0.1**: ë¶€ì ì ˆí•œ ë‹µë³€ (ì™„ì „íˆ ë¶€ì •í™•í•˜ê±°ë‚˜ ê´€ë ¨ ì—†ìŒ)
            
            * **í‰ê°€ ëª¨ë¸**:
              - LLM: Claude 3 Haiku/Sonnet/Opus
              - ì„ë² ë”©: SentenceTransformer (all-MiniLM-L6-v2)
              - ì¢…í•© ì ìˆ˜: 5ê°œ ë©”íŠ¸ë¦­ì˜ í‰ê· ê°’
            """
            )

        # ì–¸ì–´ ì„¤ì •
        st.subheader("ì–¸ì–´ ì„¤ì •")
        lang_option = st.radio("ì–¸ì–´", ["í•œêµ­ì–´", "English"], index=0)

        # í°íŠ¸ ì„¤ì •
        st.subheader("í°íŠ¸ ì„¤ì •")
        if st.button("í•œê¸€ í°íŠ¸ ì„¤ì •"):
            has_font = setup_korean_font()
            if has_font:
                st.success("í•œê¸€ í°íŠ¸ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            else:
                st.error("í•œê¸€ í°íŠ¸ ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

        # ì•± ì •ë³´
        st.subheader("ì•± ì •ë³´")
        st.info(
            """
        **OpenEvals í‰ê°€ ëŒ€ì‹œë³´ë“œ**
        
        ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ OpenEvalsë¥¼ ì‚¬ìš©í•˜ì—¬ 
        ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.
        
        GitHub: [OpenEvals](https://github.com/openevals/openevals)
        """
        )


if __name__ == "__main__":
    setup_korean_font()
    main()
