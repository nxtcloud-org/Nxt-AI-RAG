import boto3
import streamlit as st
from langchain_aws import ChatBedrock
from langchain_aws import BedrockEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
import chromadb

# ì‚¬ì´ë“œë°” ìë™ ìˆ¨ê¹€ ì„¤ì •
st.set_page_config(initial_sidebar_state="collapsed")

st.title("ğŸ” í•™ì‚¬ ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œ")
st.caption("RAG(Retrieval-Augmented Generation) ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰")


# AWS Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
@st.cache_resource
def init_bedrock():
    bedrock_client = boto3.client("bedrock-runtime", region_name="us-east-1")
    bedrock = ChatBedrock(
        client=bedrock_client,
        model_id="anthropic.claude-3-haiku-20240307-v1:0",
        model_kwargs={"anthropic_version": "bedrock-2023-05-31"},
    )
    embeddings = BedrockEmbeddings(region_name="us-east-1")
    return bedrock, embeddings


bedrock, embeddings = init_bedrock()


# PDF ë¡œë“œ ë° ì²­í¬ ë¶„í• 
@st.cache_resource
def load_and_process_pdf():
    chroma_client = chromadb.PersistentClient(path="./vector_db")

    # íŒŒì¼ë¡œë“œ
    pdf_loader = PyPDFLoader("./data/univ-data.pdf")

    # ì²­í¬ ë¶„í• 
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=500,
        chunk_overlap=50,
    )

    data = pdf_loader.load_and_split(text_splitter=splitter)

    # ë²¡í„° ìŠ¤í† ì–´ êµ¬ì„±
    vectorstore = Chroma.from_documents(
        documents=data,
        embedding=embeddings,
        persist_directory="./vector_db/",
        collection_name="university_docs",
    )

    return vectorstore


# ë©”ì¸ ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤
try:
    vectorstore = load_and_process_pdf()
    st.success("ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

    st.header("ğŸ“š í•™ì‚¬ ì •ë³´ ê²€ìƒ‰")
    search_query = st.text_input(
        "ê¶ê¸ˆí•œ ë‚´ìš©ì„ ìì—°ì–´ë¡œ ì…ë ¥í•˜ì„¸ìš”:",
        placeholder="ì˜ˆ: ì¡¸ì—…ìš”ê±´ì´ ë­ì•¼?",
        key="search_query",
    )

    if st.button("ê²€ìƒ‰", key="search_button"):
        with st.spinner("ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤..."):
            results = vectorstore.similarity_search(search_query)

            # ì¤‘ë³µ ì œê±°
            seen_contents = set()
            unique_results = []

            for doc in results:
                content = doc.page_content.strip()
                if content not in seen_contents:
                    seen_contents.add(content)
                    unique_results.append(doc)

            st.write(f"ğŸ¯ ê²€ìƒ‰ ê²°ê³¼: {len(unique_results)}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")

            for i, doc in enumerate(unique_results, 1):
                with st.expander(f"ê²€ìƒ‰ ê²°ê³¼ #{i}"):
                    st.markdown(f"**ë‚´ìš©:**\n{doc.page_content}")
                    st.caption(f"ì¶œì²˜: {doc.metadata.get('page', 'N/A')}í˜ì´ì§€")

except Exception as e:
    st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    st.error("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. 'vector_db' ë””ë ‰í† ë¦¬ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
